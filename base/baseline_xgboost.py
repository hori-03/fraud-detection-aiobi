2

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, ParameterGrid
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import numpy as np
from imblearn.over_sampling import SMOTE
from tqdm import tqdm
import time
import os


# Charger le dataset avec optimisations - DATASET30 (LOAN DEFAULT FRAUD)
# === CONFIGURATION DATASET ===
DATA_PATH = 'data/datasets/Dataset30.csv'
DATASET_NAME = os.path.splitext(os.path.basename(DATA_PATH))[0]  # Extrait "Dataset30"
PARQUET_PATH = f'data/parquet/{DATASET_NAME}.parquet'

print("Chargement du dataset...")
start_time = time.time()

# Utiliser Parquet si disponible (plus rapide)
if os.path.exists(PARQUET_PATH):
    print("Chargement depuis Parquet (optimis√©)")
    df = pd.read_parquet(PARQUET_PATH)
else:
    print("Chargement depuis CSV et conversion Parquet")
    df = pd.read_csv(DATA_PATH)
    # Sauvegarder en Parquet pour les prochaines fois
    df.to_parquet(PARQUET_PATH, index=False)
    print(f"Fichier Parquet sauvegard√© : {PARQUET_PATH}")

print(f"Dataset charg√© en {time.time() - start_time:.2f}s")


# Afficher un aper√ßu
df.info()
print(df.head())

# Afficher les premi√®res valeurs brutes de fraude (Dataset30 - target: 'default_fraud_flag')
print("Premi√®res valeurs brutes de default_fraud_flag :", df['default_fraud_flag'].head(20).tolist())
print("Colonnes disponibles:", df.columns.tolist())

# Gestion des valeurs manquantes
df = df.fillna(df.median(numeric_only=True))

# FEATURE ENGINEERING D'ABORD (avant encodage) - Dataset30 (LOAN DEFAULT FRAUD)
print("Feature engineering pr√©liminaire (avant encodage) - Dataset30...")

# Features credit risk (Dataset30 a debt_to_income_ratio, number_of_delinquencies, annual_income_k_usd)
if 'debt_to_income_ratio' in df.columns:
    df['debt_income_log'] = np.log1p(df['debt_to_income_ratio'])
    df['is_high_debt'] = (df['debt_to_income_ratio'] > df['debt_to_income_ratio'].quantile(0.75)).astype(int)
    df['is_low_debt'] = (df['debt_to_income_ratio'] < df['debt_to_income_ratio'].quantile(0.25)).astype(int)

if 'number_of_delinquencies' in df.columns:
    df['has_delinquency'] = (df['number_of_delinquencies'] > 0).astype(int)
    df['delinquency_log'] = np.log1p(df['number_of_delinquencies'])

if 'annual_income_k_usd' in df.columns:
    df['income_log'] = np.log1p(df['annual_income_k_usd'])
    df['is_low_income'] = (df['annual_income_k_usd'] < df['annual_income_k_usd'].quantile(0.25)).astype(int)
    if 'loan_amount_fcfa' in df.columns:
        # Ratio loan/income (converti en m√™me unit√©)
        df['loan_to_income_ratio'] = df['loan_amount_fcfa'] / (df['annual_income_k_usd'] * 1000 + 1)

# Encodage des variables cat√©gorielles avec optimisation (Dataset30 - LOAN DEFAULT FRAUD)
print("Encodage des variables cat√©gorielles...")
# Dataset30 utilise 'default_fraud_flag' comme target, et a: loan_transaction_id, borrower_id, tx_timestamp
categorical_cols = [col for col in df.select_dtypes(include=['object']).columns 
                    if col not in ['default_fraud_flag', 'loan_transaction_id', 'borrower_id', 'tx_timestamp']]

for col in categorical_cols:
    print(f"  Encodage de {col}...")
    df[col] = LabelEncoder().fit_transform(df[col].astype(str))

# Feature engineering avanc√© Dataset30 (LOAN DEFAULT FRAUD)
print("Feature engineering avanc√© pour Dataset30...")

# Features temporelles depuis tx_timestamp (Dataset30 a tx_timestamp)
if 'tx_timestamp' in df.columns and df['tx_timestamp'].dtype == 'object':
    # Parser tx_timestamp - Dataset30
    df['timestamp'] = pd.to_datetime(df['tx_timestamp'], errors='coerce')
    
    # Features temporelles Dataset30
    df['transaction_hour'] = df['timestamp'].dt.hour
    df['transaction_day'] = df['timestamp'].dt.day
    df['transaction_month'] = df['timestamp'].dt.month
    df['transaction_weekday'] = df['timestamp'].dt.weekday
    df['transaction_is_weekend'] = (df['transaction_weekday'] >= 5).astype(int)
    df['is_business_hours'] = ((df['transaction_hour'] >= 8) & (df['transaction_hour'] <= 18)).astype(int)
    df['is_night'] = ((df['transaction_hour'] >= 22) | (df['transaction_hour'] <= 6)).astype(int)
    df['is_early_morning'] = ((df['transaction_hour'] >= 0) & (df['transaction_hour'] <= 6)).astype(int)
elif 'date_transaction' in df.columns and df['date_transaction'].dtype == 'object':
    # Parser date_transaction - Dataset19
    df['timestamp'] = pd.to_datetime(df['date_transaction'], errors='coerce')
    
    # Features temporelles Dataset19 (hour pr√©-extrait, PAS de weekday)
    df['transaction_hour'] = df['hour'] if 'hour' in df.columns else df['timestamp'].dt.hour
    df['transaction_day'] = df['timestamp'].dt.day
    df['transaction_month'] = df['timestamp'].dt.month
    df['transaction_weekday'] = df['timestamp'].dt.weekday  # Dataset19 n'a PAS weekday pr√©-extrait
    df['transaction_is_weekend'] = (df['transaction_weekday'] >= 5).astype(int)
    df['is_business_hours'] = ((df['transaction_hour'] >= 8) & (df['transaction_hour'] <= 18)).astype(int)
    df['is_night'] = ((df['transaction_hour'] >= 22) | (df['transaction_hour'] <= 6)).astype(int)
    df['is_early_morning'] = ((df['transaction_hour'] >= 0) & (df['transaction_hour'] <= 6)).astype(int)
elif 'tx_timestamp' in df.columns and df['tx_timestamp'].dtype == 'object':
    # Parser tx_timestamp - Dataset18
    df['timestamp'] = pd.to_datetime(df['tx_timestamp'], errors='coerce')
    
    # Features temporelles Dataset18 (hour et weekday DEJA extraits)
    df['transaction_hour'] = df['hour'] if 'hour' in df.columns else df['timestamp'].dt.hour
    df['transaction_day'] = df['timestamp'].dt.day
    df['transaction_month'] = df['timestamp'].dt.month
    df['transaction_weekday'] = df['weekday'] if 'weekday' in df.columns else df['timestamp'].dt.weekday
    df['transaction_is_weekend'] = (df['transaction_weekday'] >= 5).astype(int)
    df['is_business_hours'] = ((df['transaction_hour'] >= 8) & (df['transaction_hour'] <= 18)).astype(int)
    df['is_night'] = ((df['transaction_hour'] >= 22) | (df['transaction_hour'] <= 6)).astype(int)
    df['is_early_morning'] = ((df['transaction_hour'] >= 0) & (df['transaction_hour'] <= 6)).astype(int)
elif 'datetime_tx' in df.columns:
    # Dataset17 a datetime_tx (√† parser) + hour ET weekday pr√©-extraits
    df['timestamp'] = pd.to_datetime(df['datetime_tx'], errors='coerce')
    
    # Features temporelles Dataset17 (hour et weekday DEJA extraits)
    df['transaction_hour'] = df['hour'] if 'hour' in df.columns else df['timestamp'].dt.hour
    df['transaction_day'] = df['timestamp'].dt.day
    df['transaction_month'] = df['timestamp'].dt.month
    df['transaction_weekday'] = df['weekday'] if 'weekday' in df.columns else df['timestamp'].dt.weekday
    df['transaction_is_weekend'] = (df['transaction_weekday'] >= 5).astype(int)
    df['is_business_hours'] = ((df['transaction_hour'] >= 8) & (df['transaction_hour'] <= 18)).astype(int)
    df['is_night'] = ((df['transaction_hour'] >= 22) | (df['transaction_hour'] <= 6)).astype(int)
    df['is_early_morning'] = ((df['transaction_hour'] >= 0) & (df['transaction_hour'] <= 6)).astype(int)
elif 'hour' in df.columns and 'weekday' in df.columns:
    # Dataset15 a hour et weekday pr√©-extraits
    df['transaction_hour'] = df['hour']
    df['transaction_weekday'] = df['weekday']
    df['transaction_is_weekend'] = (df['transaction_weekday'] >= 5).astype(int) if 'is_weekend' not in df.columns else df['is_weekend']
    df['is_business_hours'] = ((df['transaction_hour'] >= 8) & (df['transaction_hour'] <= 18)).astype(int)
    df['is_night'] = ((df['transaction_hour'] >= 22) | (df['transaction_hour'] <= 6)).astype(int)
    df['is_early_morning'] = ((df['transaction_hour'] >= 0) & (df['transaction_hour'] <= 6)).astype(int)

# Features amount (Dataset24 utilise 'amount_fcfa')
if 'amount_fcfa' in df.columns:
    df['amount_log'] = np.log1p(df['amount_fcfa'])
    df['amount_squared'] = df['amount_fcfa'] ** 2
    df['amount_sqrt'] = np.sqrt(df['amount_fcfa'])
elif 'withdrawal_amount_fcfa' in df.columns:
    df['amount_log'] = np.log1p(df['withdrawal_amount_fcfa'])
    df['amount_squared'] = df['withdrawal_amount_fcfa'] ** 2
    df['amount_sqrt'] = np.sqrt(df['withdrawal_amount_fcfa'])
elif 'wire_amount_fcfa' in df.columns:
    df['amount_log'] = np.log1p(df['wire_amount_fcfa'])
    df['amount_squared'] = df['wire_amount_fcfa'] ** 2
    df['amount_sqrt'] = np.sqrt(df['wire_amount_fcfa'])
elif 'transaction_amount_fcfa' in df.columns:
    df['amount_log'] = np.log1p(df['transaction_amount_fcfa'])
    df['amount_squared'] = df['transaction_amount_fcfa'] ** 2
    df['amount_sqrt'] = np.sqrt(df['transaction_amount_fcfa'])
elif 'loan_amount_fcfa' in df.columns:
    df['amount_log'] = np.log1p(df['loan_amount_fcfa'])
    df['amount_squared'] = df['loan_amount_fcfa'] ** 2
    df['amount_sqrt'] = np.sqrt(df['loan_amount_fcfa'])
elif 'tx_amount_xof' in df.columns:
    df['amount_log'] = np.log1p(df['tx_amount_xof'])
    df['amount_squared'] = df['tx_amount_xof'] ** 2
    df['amount_sqrt'] = np.sqrt(df['tx_amount_xof'])

# Binning amount (Dataset28 utilise 'claim_amount_fcfa')
if 'claim_amount_fcfa' in df.columns:
    df['amount_log'] = np.log1p(df['claim_amount_fcfa'])
    df['amount_zscore'] = (df['claim_amount_fcfa'] - df['claim_amount_fcfa'].mean()) / df['claim_amount_fcfa'].std()
    df['amount_bin'] = pd.cut(df['claim_amount_fcfa'], bins=10, labels=False)
    df['is_high_amount'] = (df['claim_amount_fcfa'] > df['claim_amount_fcfa'].quantile(0.95)).astype(int)
    df['is_low_amount'] = (df['claim_amount_fcfa'] < df['claim_amount_fcfa'].quantile(0.05)).astype(int)
    df['is_round_amount'] = (df['claim_amount_fcfa'] % 10000 == 0).astype(int)
elif 'trade_value_fcfa' in df.columns:
    df['amount_log'] = np.log1p(df['trade_value_fcfa'])
    df['amount_zscore'] = (df['trade_value_fcfa'] - df['trade_value_fcfa'].mean()) / df['trade_value_fcfa'].std()
    df['amount_bin'] = pd.cut(df['trade_value_fcfa'], bins=10, labels=False)
    df['is_high_amount'] = (df['trade_value_fcfa'] > df['trade_value_fcfa'].quantile(0.95)).astype(int)
    df['is_low_amount'] = (df['trade_value_fcfa'] < df['trade_value_fcfa'].quantile(0.05)).astype(int)
    df['is_round_amount'] = (df['trade_value_fcfa'] % 10000 == 0).astype(int)
elif 'monthly_payment_fcfa' in df.columns:
    df['amount_log'] = np.log1p(df['monthly_payment_fcfa'])
    df['amount_zscore'] = (df['monthly_payment_fcfa'] - df['monthly_payment_fcfa'].mean()) / df['monthly_payment_fcfa'].std()
    df['amount_bin'] = pd.cut(df['monthly_payment_fcfa'], bins=10, labels=False)
    df['is_high_amount'] = (df['monthly_payment_fcfa'] > df['monthly_payment_fcfa'].quantile(0.95)).astype(int)
    df['is_low_amount'] = (df['monthly_payment_fcfa'] < df['monthly_payment_fcfa'].quantile(0.05)).astype(int)
    df['is_round_amount'] = (df['monthly_payment_fcfa'] % 10000 == 0).astype(int)
elif 'amount_fcfa' in df.columns:
    df['amount_bin'] = pd.cut(df['amount_fcfa'], bins=10, labels=False)
    # is_high_amount peut d√©j√† exister dans Dataset24, recr√©ons si absent
    if 'is_high_amount' not in df.columns:
        df['is_high_amount'] = (df['amount_fcfa'] > df['amount_fcfa'].quantile(0.95)).astype(int)
    df['is_low_amount'] = (df['amount_fcfa'] < df['amount_fcfa'].quantile(0.05)).astype(int)
    df['is_round_amount'] = (df['amount_fcfa'] % 10000 == 0).astype(int)
elif 'withdrawal_amount_fcfa' in df.columns:
    df['amount_bin'] = pd.cut(df['withdrawal_amount_fcfa'], bins=10, labels=False)
    # is_high_amount existe d√©j√† dans Dataset22, mais recr√©ons pour coh√©rence
    if 'is_high_amount' not in df.columns:
        df['is_high_amount'] = (df['withdrawal_amount_fcfa'] > df['withdrawal_amount_fcfa'].quantile(0.95)).astype(int)
    df['is_low_amount'] = (df['withdrawal_amount_fcfa'] < df['withdrawal_amount_fcfa'].quantile(0.05)).astype(int)
    df['is_round_amount'] = (df['withdrawal_amount_fcfa'] % 10000 == 0).astype(int)
elif 'wire_amount_fcfa' in df.columns:
    df['amount_bin'] = pd.cut(df['wire_amount_fcfa'], bins=10, labels=False)
    if 'is_high_amount' not in df.columns:
        df['is_high_amount'] = (df['wire_amount_fcfa'] > df['wire_amount_fcfa'].quantile(0.95)).astype(int)
    df['is_low_amount'] = (df['wire_amount_fcfa'] < df['wire_amount_fcfa'].quantile(0.05)).astype(int)
    df['is_round_amount'] = (df['wire_amount_fcfa'] % 10000 == 0).astype(int)
elif 'transaction_amount_fcfa' in df.columns:
    df['amount_bin'] = pd.cut(df['transaction_amount_fcfa'], bins=10, labels=False)
    if 'is_high_amount' not in df.columns:
        df['is_high_amount'] = (df['transaction_amount_fcfa'] > df['transaction_amount_fcfa'].quantile(0.95)).astype(int)
    df['is_low_amount'] = (df['transaction_amount_fcfa'] < df['transaction_amount_fcfa'].quantile(0.05)).astype(int)
    df['is_round_amount'] = (df['transaction_amount_fcfa'] % 10000 == 0).astype(int)
elif 'loan_amount_fcfa' in df.columns:
    df['amount_bin'] = pd.cut(df['loan_amount_fcfa'], bins=10, labels=False)
    if 'is_high_amount' not in df.columns:
        df['is_high_amount'] = (df['loan_amount_fcfa'] > df['loan_amount_fcfa'].quantile(0.95)).astype(int)
    df['is_low_amount'] = (df['loan_amount_fcfa'] < df['loan_amount_fcfa'].quantile(0.05)).astype(int)
    df['is_round_amount'] = (df['loan_amount_fcfa'] % 10000 == 0).astype(int)
elif 'tx_amount_xof' in df.columns:
    df['amount_bin'] = pd.cut(df['tx_amount_xof'], bins=10, labels=False)
    df['is_high_amount'] = (df['tx_amount_xof'] > df['tx_amount_xof'].quantile(0.95)).astype(int)
    df['is_low_amount'] = (df['tx_amount_xof'] < df['tx_amount_xof'].quantile(0.05)).astype(int)
    df['is_round_amount'] = (df['tx_amount_xof'] % 10000 == 0).astype(int)

# Features √¢ge client et anciennet√© compte (Dataset20 utilise 'cust_age' et 'account_tenure_days')
if 'cust_age' in df.columns:
    df['age_log'] = np.log1p(df['cust_age'])
    df['is_young_customer'] = (df['cust_age'] < 25).astype(int)
    df['is_senior_customer'] = (df['cust_age'] > 60).astype(int)

if 'account_tenure_days' in df.columns:
    df['tenure_log'] = np.log1p(df['account_tenure_days'])
    df['is_new_account'] = (df['account_tenure_days'] < 30).astype(int)
    df['is_old_account'] = (df['account_tenure_days'] > 365).astype(int)

# Features sp√©cifiques carte bancaire (Dataset20)
if 'transaction_velocity_24h' in df.columns:
    df['velocity_log'] = np.log1p(df['transaction_velocity_24h'])
    df['is_high_velocity'] = (df['transaction_velocity_24h'] > df['transaction_velocity_24h'].quantile(0.90)).astype(int)

if 'distance_from_home' in df.columns:
    df['distance_log'] = np.log1p(df['distance_from_home'])
    df['is_far_from_home'] = (df['distance_from_home'] > df['distance_from_home'].quantile(0.90)).astype(int)
    df['is_very_far'] = (df['distance_from_home'] > df['distance_from_home'].quantile(0.95)).astype(int)



# Interactions entre features (Dataset20 utilise merchant_category, transaction_amount_fcfa)
if 'transaction_hour' in df.columns and 'amount_log' in df.columns:
    df['hour_amount_interaction'] = df['transaction_hour'] * df['amount_log']

if 'merchant_category' in df.columns and 'amount_log' in df.columns:
    # merchant_category est d√©j√† encod√© num√©riquement (Dataset20)
    df['merchant_amount_interaction'] = df['merchant_category'] * df['amount_log']
elif 'cust_region' in df.columns and 'amount_log' in df.columns:
    # cust_region est d√©j√† encod√© num√©riquement (Dataset19)
    df['region_amount_interaction'] = df['cust_region'] * df['amount_log']
elif 'province' in df.columns and 'amount_log' in df.columns:
    # province est d√©j√† encod√© num√©riquement (Dataset18)
    df['province_amount_interaction'] = df['province'] * df['amount_log']

# Interactions sp√©cifiques carte bancaire (Dataset20)
if 'is_foreign_currency' in df.columns and 'is_international' in df.columns:
    df['foreign_and_international'] = df['is_foreign_currency'] * df['is_international']

if 'is_night' in df.columns and 'is_high_amount' in df.columns:
    df['night_and_high_amount'] = df['is_night'] * df['is_high_amount']
elif 'is_night_tx' in df.columns and 'is_high_amount' in df.columns:
    df['night_and_high_amount'] = df['is_night_tx'] * df['is_high_amount']

if 'transaction_velocity_24h' in df.columns and 'transaction_amount_fcfa' in df.columns:
    df['velocity_x_amount'] = df['transaction_velocity_24h'] * df['wire_amount_fcfa'] / 1000000
elif 'transaction_velocity_24h' in df.columns and 'transaction_amount_fcfa' in df.columns:
    df['velocity_x_amount'] = df['transaction_velocity_24h'] * df['transaction_amount_fcfa'] / 1000000
elif 'transaction_velocity_24h' in df.columns and 'amount_log' in df.columns:
    df['velocity_x_amount_log'] = df['transaction_velocity_24h'] * df['amount_log']

if 'tx_method' in df.columns and 'amount_log' in df.columns:
    # tx_method est d√©j√† encod√© num√©riquement
    df['method_amount_interaction'] = df['tx_method'] * df['amount_log']

# Bruitage pour features quantitatives (Dataset24 utilise amount_fcfa)
np.random.seed(42)
if 'amount_fcfa' in df.columns:
    df['amount_noisy'] = df['amount_fcfa'] + np.random.normal(0, 0.01 * df['amount_fcfa'].std(), len(df))
elif 'withdrawal_amount_fcfa' in df.columns:
    df['amount_noisy'] = df['withdrawal_amount_fcfa'] + np.random.normal(0, 0.01 * df['withdrawal_amount_fcfa'].std(), len(df))
elif 'wire_amount_fcfa' in df.columns:
    df['amount_noisy'] = df['wire_amount_fcfa'] + np.random.normal(0, 0.01 * df['wire_amount_fcfa'].std(), len(df))
elif 'transaction_amount_fcfa' in df.columns:
    df['amount_noisy'] = df['transaction_amount_fcfa'] + np.random.normal(0, 0.01 * df['transaction_amount_fcfa'].std(), len(df))
elif 'loan_amount_fcfa' in df.columns:
    df['amount_noisy'] = df['loan_amount_fcfa'] + np.random.normal(0, 0.01 * df['loan_amount_fcfa'].std(), len(df))
elif 'amount_fcfa' in df.columns:
    df['amount_noisy'] = df['amount_fcfa'] + np.random.normal(0, 0.01 * df['amount_fcfa'].std(), len(df))
elif 'tx_amount_xof' in df.columns:
    df['amount_noisy'] = df['tx_amount_xof'] + np.random.normal(0, 0.01 * df['tx_amount_xof'].std(), len(df))

# La colonne cible dans Dataset30 est 'default_fraud_flag' (0/1)
print("Valeurs uniques dans default_fraud_flag :", df['default_fraud_flag'].unique())
print("Distribution default_fraud_flag :", df['default_fraud_flag'].value_counts().to_dict())
print("Nombre de NaN dans default_fraud_flag :", df['default_fraud_flag'].isna().sum())

# default_fraud_flag est d√©j√† num√©rique (0/1), pas besoin de conversion
df['target'] = df['default_fraud_flag'].astype(int)
print("Apr√®s encodage - Distribution target :", df['target'].value_counts().to_dict())

# Supprimer les lignes o√π la cible est NaN (si applicable)
df = df[df['target'].notna()]

# Exclure les colonnes non-num√©riques AVANT la s√©paration X/y (Dataset30 - Loan Default Fraud)
print("Nettoyage des colonnes avant entra√Ænement...")
columns_to_exclude = [
    'target',                      # Target encod√©
    'default_fraud_flag',          # Target original (Dataset30)
    'chargeback_fraud',            # Target original (Dataset29)
    'fraud_indicator',             # Target original (Dataset28)
    'market_manipulation_flag',    # Target original (Dataset27)
    'payment_irregularity',        # Target original (Dataset26)
    'suspicious_activity',         # Target original (Dataset25)
    'fraud_alert',                 # Target original (Dataset24)
    'aml_flagged',                 # Target original (Dataset23)
    'skimming_detected',           # Target original (Dataset22)
    'flagged_suspicious',          # Target original (Dataset21)
    'is_fraudulent_transaction',   # Target original (Dataset20, autres)
    'is_fraudulent',               # Target original (Dataset19, autres)
    'loan_transaction_id',         # ID de transaction (Dataset30)
    'borrower_id',                 # ID emprunteur (Dataset30)
    'pos_transaction_id',          # ID de transaction (Dataset29)
    'merchant_id',                 # ID marchand (Dataset29)
    'terminal_id',                 # ID terminal (Dataset29)
    'claim_number',                # ID de r√©clamation (Dataset28)
    'policy_holder_id',            # ID titulaire police (Dataset28)
    'trade_order_id',              # ID de transaction (Dataset27)
    'brokerage_account_id',        # ID compte courtier (Dataset27)
    'payment_reference',           # ID de transaction (Dataset26)
    'loan_account_id',             # ID compte de pr√™t (Dataset26)
    'crypto_tx_hash',              # ID de transaction (Dataset25)
    'wallet_address_hash',         # Hash de wallet (Dataset25)
    'session_transaction_id',      # ID de transaction (Dataset24)
    'user_id_hash',                # Hash d'utilisateur (Dataset24)
    'payment_order_id',            # ID de transaction (Dataset23)
    'corporate_client_id',         # ID client corporatif (Dataset23)
    'atm_transaction_ref',         # ID de transaction (Dataset22)
    'card_hash',                   # Hash de carte (Dataset22)
    'wire_reference',              # ID de transaction (Dataset21)
    'account_iban_hash',           # Hash de compte (Dataset21)
    'card_transaction_id',         # ID de transaction (Dataset20)
    'card_number_hash',            # Hash de carte (Dataset20)
    'tx_id',                       # ID de transaction (Dataset19, autres)
    'customer_ref',                # ID client (Dataset19, autres)
    'tx_timestamp',                # Timestamp original (Dataset20, 25, 26 - on garde les features d√©riv√©es)
    'date_transaction',            # Date originale (Dataset19, 24 - on garde les features d√©riv√©es)
    'heure_transaction',           # Time original (Dataset19, 24 - on garde les features d√©riv√©es)
    'datetime_tx',                 # Datetime original (autres datasets)
    'timestamp'                    # Datetime pars√© (on garde les features d√©riv√©es)
]

# Garder toutes les colonnes SAUF celles explicitement exclues
feature_columns = []
for col in df.columns:
    if col not in columns_to_exclude:
        # Convertir en num√©rique si ce n'est pas d√©j√† fait
        if df[col].dtype == 'object':
            try:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].fillna(0)  # Remplacer NaN par 0
                print(f"Conversion de {col} en num√©rique")
            except:
                print(f"Exclusion de {col} (non convertible)")
                continue
        
        feature_columns.append(col)
        print(f"Gard√©: {col} (type: {df[col].dtype})")

print(f"Features retenues: {len(feature_columns)}")
print(f"Features importantes Dataset10: {[col for col in feature_columns if col in ['payment_method', 'transaction_purpose', 'customer_type', 'zone', 'merchant_category', 'device_type']]}")

# S√©parer features et label
X = df[feature_columns]
y = df['target']

print(f"Forme finale de X: {X.shape}")
print(f"Types de donn√©es dans X:\n{X.dtypes.value_counts()}")



# Split train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Gestion du d√©s√©quilibre des classes
scale_pos_weight = np.sum(y_train == 0) / np.sum(y_train == 1)

# üîÑ RECHERCHE PAR √âTAPES POUR FRAUD DETECTION
# √âTAPE 1: Grille grossi√®re pour identifier la meilleure zone
# √âTAPE 2: Fine-tuning pr√©cis autour des meilleurs r√©sultats

# üìä ADAPTATION SP√âCIFIQUE Dataset25 (15K lignes + Fraud 10.24% - CRYPTOCURRENCY TRANSACTION MONITORING)
print(f"\nüéØ ANALYSE {DATASET_NAME} POUR OPTIMISATION GRILLE:")
print(f"   ‚Ä¢ Features: ~{len(X_train.columns)} (features engineered + Mortgage/Payment specific)")
print(f"   ‚Ä¢ Fraud rate: {y_train.mean()*100:.2f}% ({y_train.sum()} cas) ‚Üí TR√àS FAIBLE (imbalance 70.3:1)")
print(f"   ‚Ä¢ Scale pos weight optimal: {scale_pos_weight:.1f}:1")
print(f"   ‚Ä¢ Type: Taille moyenne (42K lignes - MORTGAGE/SUBSCRIPTION PAYMENT FRAUD)")
print(f"   ‚Ä¢ Strat√©gie: arbres profonds, learning lent, scale_pos_weight ~{int(scale_pos_weight)}")

# üìä √âTAPE 1: GRILLE OPTIMIS√âE - Dataset30 (42K lignes, 3.38% fraud)
param_grid_stage1 = {
    # Exploration OPTIMIS√âE Dataset30 - Taille moyenne avec Fraud MOD√âR√â
    'max_depth': [6, 8],                      # OPTIMIS√â: 42K lignes + fraud 3.38% ‚Üí arbres moyennement profonds
    'learning_rate': [0.05, 0.08, 0.11],     # OPTIMIS√â: Apprentissage mod√©r√© pour classe mod√©r√©e
    'subsample': [0.75, 0.85],               # OPTIMIS√â: Subsampling mod√©r√©-√©lev√© pour 42K
    'min_child_weight': [3, 5],              # OPTIMIS√â: 1,419 frauds (mod√©r√©) ‚Üí feuilles moyennes
    'gamma': [0.1, 0.2],                     # OPTIMIS√â: 3.38% fraud (mod√©r√©) ‚Üí pruning mod√©r√©-√©lev√©
    
    # Param√®tres secondaires - OPTIMIS√âS Dataset30
    'colsample_bytree': [0.70, 0.80],        # OPTIMIS√â: 21 colonnes source ‚Üí 70-80% des features
    'reg_alpha': [0.0, 0.1],                 # OPTIMIS√â: L1 faible-mod√©r√©e (classe mod√©r√©e)
    'reg_lambda': [0.8, 1.5],                # OPTIMIS√â: L2 mod√©r√©e-√©lev√©e pour 42K lignes
    'n_estimators': [400],                   # ADAPT√â: Arbres nombreux (taille moyenne + classe mod√©r√©e)
    
    # Scale pos weight - OPTIMAL pour imbalance ~28.6:1 (MOD√âR√â)
    'scale_pos_weight': [
        scale_pos_weight * 0.85,             # Sous-p√©nalisation
        scale_pos_weight,                    # Optimal calcul√© (~29)
        scale_pos_weight * 1.15              # Sur-p√©nalisation
    ]
}

# Total √âtape 1: 2√ó3√ó2√ó2√ó2√ó2√ó2√ó2√ó1√ó3 = 1,152 combinaisons sur 35% donn√©es
param_grid = param_grid_stage1

# STRAT√âGIE OPTIMIS√âE 2-√âTAPES POUR Dataset30:
# Stage 1: 1,152 combinaisons sur 35% donn√©es ‚Üí Exploration √©quilibr√©e (~20-30 min)
# Stage 2: ~2,000-3,000 combinaisons sur 100% donn√©es ‚Üí Fine-tuning pr√©cis (~80-120 min)
total_stage1 = 2*3*2*2*2*2*2*2*1*3
print(f"\nüîÑ RECHERCHE PAR √âTAPES - {DATASET_NAME} (TAILLE MOYENNE - 42K LIGNES):")
print(f"   Stage 1 (exploration): {total_stage1} combinaisons sur 35% donn√©es (~20-30 min)")
print(f"   Stage 2 (fine-tuning): ~2,000-3,000 combinaisons sur 100% donn√©es (~80-120 min)")
print(f"   Temps total estim√©: 100-150 minutes (1.7-2.5 heures)")
print(f"   Performance attendue: 98-99% de l'optimal")
print(f"üí° Grille adapt√©e aux caract√©ristiques {DATASET_NAME} (Loan Default Fraud + classe MOD√âR√âE)")
print("‚ö° Strat√©gie optimis√©e: exploration rapide puis fine-tuning complet")
print()
print(f"üîç OPTIMISATIONS SP√âCIFIQUES {DATASET_NAME} (42K LIGNES, {y_train.mean()*100:.2f}% FRAUD MOD√âR√â):")
print(f"   ‚Ä¢ max_depth [6,8] ‚Üí Taille 42K + fraud 3.38% ‚Üí arbres MOYENS-PROFONDS")
print(f"   ‚Ä¢ learning_rate [0.05-0.11] ‚Üí Convergence mod√©r√©e pour classe mod√©r√©e")
print(f"   ‚Ä¢ subsample [0.75-0.85] ‚Üí Sous-√©chantillonnage MOD√âR√â-√âLEV√â (42K lignes)")
print(f"   ‚Ä¢ min_child_weight [3,5] ‚Üí {y_train.sum()} frauds (mod√©r√©) ‚Üí feuilles moyennes")
print(f"   ‚Ä¢ gamma [0.1-0.2] ‚Üí Pruning mod√©r√©-√©lev√© pour classe mod√©r√©e")
print(f"   ‚Ä¢ colsample_bytree [0.70-0.80] ‚Üí Garder 70-80% des ~{len(X_train.columns)} features")
print(f"   ‚Ä¢ reg_alpha [0.0-0.1] ‚Üí L1 faible-mod√©r√©e (classe mod√©r√©e)")
print(f"   ‚Ä¢ reg_lambda [0.8-1.5] ‚Üí L2 mod√©r√©e-√©lev√©e pour g√©n√©ralisation")
print(f"   ‚Ä¢ scale_pos_weight 3 VALEURS ‚Üí Imbalance {scale_pos_weight:.2f}:1 (MOD√âR√â)")
print(f"   ‚Ä¢ n_estimators [400] ‚Üí Arbres nombreux (taille moyenne + classe mod√©r√©e)")


# Initialiser le mod√®le XGBoost directement
model = xgb.XGBClassifier(random_state=42)

# Validation crois√©e stratifi√©e avec plus de robustesse
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# GridSearchCV avec multiple scoring pour plus de diversit√©
print(f"D√©marrage GridSearchCV avec {len(list(ParameterGrid(param_grid)))} combinaisons...")
print(f"Utilisation de {cv.n_splits} folds de validation crois√©e")

# Utiliser diff√©rentes m√©triques pour avoir plus de vari√©t√© dans le classement
scoring_metrics = ['f1', 'precision', 'recall', 'roc_auc']

# üîÑ RECHERCHE PAR √âTAPES IMPL√âMENTATION
n_jobs = os.cpu_count() or 1
print(f"üöÄ Parall√©lisation sur {n_jobs} CPU")

print(f"\nüéØ === √âTAPE 1/2: EXPLORATION LARGE (35% donn√©es) ===")
print(f"Grille grossi√®re: {len(list(ParameterGrid(param_grid)))} combinaisons")
print(f"Objectif: Identifier les meilleures zones de param√®tres RAPIDEMENT")

# STRAT√âGIE OPTIMIS√âE: Sous-√©chantillonner pour grille 1 (exploration rapide)
# Grille 1 (35% donn√©es) ‚Üí Trouve la bonne zone rapidement
# Grille 2 (100% donn√©es) ‚Üí Fine-tuning pr√©cis avec toutes les donn√©es
sample_fraction_stage1 = 0.35
n_samples_stage1 = int(len(X_train) * sample_fraction_stage1)

# √âchantillonnage stratifi√© pour pr√©server la distribution de fraude
from sklearn.model_selection import train_test_split
X_train_stage1, _, y_train_stage1, _ = train_test_split(
    X_train, y_train, 
    train_size=sample_fraction_stage1, 
    random_state=42, 
    stratify=y_train
)

print(f"üìä Donn√©es Stage 1: {len(X_train_stage1):,} lignes ({sample_fraction_stage1*100:.0f}% de {len(X_train):,})")
print(f"   Fraud rate Stage 1: {y_train_stage1.mean()*100:.2f}% (identique au train complet)")

# √âTAPE 1: Grille grossi√®re sur sous-√©chantillon
start_stage1 = time.time()
grid_stage1 = GridSearchCV(
    model, param_grid,
    scoring='f1',  # Une seule m√©trique pour stage 1
    refit=True,
    cv=cv,
    verbose=2,
    n_jobs=n_jobs,
    return_train_score=False
)

grid_stage1.fit(X_train_stage1, y_train_stage1)
stage1_time = time.time() - start_stage1

print(f"\n‚úÖ √âTAPE 1 termin√©e en {stage1_time:.1f}s")
print(f"Meilleur score Stage 1: {grid_stage1.best_score_:.4f}")
print(f"Meilleurs params Stage 1: {grid_stage1.best_params_}")

# Analyser les r√©sultats Stage 1 pour d√©finir Stage 2
results_stage1 = pd.DataFrame(grid_stage1.cv_results_)
# √âTAPE 1 utilise scoring='f1' ‚Üí colonne 'mean_test_score'
top_configs = results_stage1.nlargest(5, 'mean_test_score')

print(f"\nüîç Top 3 configurations Stage 1:")
for i, (idx, row) in enumerate(top_configs.head(3).iterrows()):
    print(f"  {i+1}. F1: {row['mean_test_score']:.4f} | {row['params']}")

# üìä √âTAPE 2: FINE-TUNING autour des meilleures configurations - ADAPT√â Dataset20
print(f"\nüéØ === √âTAPE 2/2: FINE-TUNING (Dataset20 OPTIMIS√â) ===")
print(f"Fine-tuning autour des meilleurs r√©sultats Stage 1")
print(f"Strat√©gie: Raffinement pr√©cis avec contraintes Dataset20 (BANK CARD FRAUD)")

# Extraire les meilleures valeurs de Stage 1
best_params = grid_stage1.best_params_
best_depth = best_params['max_depth']
best_lr = best_params['learning_rate']
best_subsample = best_params['subsample']
best_min_child = best_params['min_child_weight']
best_gamma = best_params['gamma']

# Construire grille Stage 2 ADAPT√âE Dataset25 (15K lignes - taille MOYENNE, fraud MOD√âR√â-√âLEV√â)
param_grid_stage2 = {
    # Fine-tuning ADAPT√â - Dataset25 avec 15K lignes (taille MOYENNE, fraud MOD√âR√â-√âLEV√â 10.24%)
    'max_depth': [
        max(3, best_depth-1),                # Min 3 (fraud mod√©r√©-√©lev√© ‚Üí profondeur moyenne)
        best_depth, 
        min(7, best_depth+1)                 # Max 7 (dataset 15K + fraud mod√©r√©-√©lev√©)
    ],
    'learning_rate': [
        max(0.07, best_lr-0.03),             # Min 0.07 (convergence rapide)
        best_lr, 
        min(0.22, best_lr+0.03)              # Max 0.22 (rapide pour classe mod√©r√©e-√©lev√©e)
    ],
    'subsample': [
        max(0.70, best_subsample-0.1),       # Min 0.70 (classe mod√©r√©e-√©lev√©e)
        best_subsample, 
        min(0.90, best_subsample+0.05)       # Max 0.90 (adapt√© 15K lignes)
    ],
    'min_child_weight': [
        max(1, best_min_child-1),            # Min 1 (feuilles petites pour 1,536 frauds)
        best_min_child, 
        min(3, best_min_child+1)             # Max 3 (1,536 frauds = MOD√âR√â)
    ],
    'gamma': [
        max(0.0, best_gamma-0.05),           # Min 0.0 (pruning minimal)
        best_gamma, 
        min(0.15, best_gamma+0.08)           # Max 0.15 (mod√©r√© - fraud MOD√âR√â-√âLEV√â)
    ],
    
    # Exploration param√®tres secondaires - OPTIMIS√âS Dataset25 (15K, 10.24% fraud, ~40-45 features)
    'colsample_bytree': [0.75, 0.85],        # OPTIMIS√â: 75-85% des ~45 features (24 colonnes source)
    'reg_alpha': [0.0, 0.05],                # OPTIMIS√â: L1 l√©ger (classe mod√©r√©e-√©lev√©e)
    'reg_lambda': [0.5, 0.8],                # OPTIMIS√â: L2 standard pour 15K lignes
    'n_estimators': [350],                   # R√âDUIT: Moins d'arbres (taille moyenne + classe mod√©r√©e-√©lev√©e)
    
    # Scale pos weight - FINE-TUNING pour imbalance ~8.8:1 (Dataset25 - MOD√âR√âMENT D√âS√âQUILIBR√â)
    'scale_pos_weight': [
        scale_pos_weight * 0.85,             # Sous-p√©nalisation (~7.5)
        scale_pos_weight,                    # Optimal (~8.8)
        scale_pos_weight * 1.15              # Sur-p√©nalisation (~10.1)
    ]
}

stage2_combinations = len(list(ParameterGrid(param_grid_stage2)))
print(f"Grille fine Stage 2 (Dataset25 - 15K lignes avec fraud MOD√âR√â-√âLEV√â): {stage2_combinations} combinaisons")
print(f"   ‚Ä¢ max_depth: 3-7 ‚Üí Profondeur MOYENNE pour 15K lignes + 10.24% fraud MOD√âR√â-√âLEV√â")
print(f"   ‚Ä¢ colsample_bytree: 0.75-0.85 (2 valeurs) ‚Üí Adapt√© aux ~45 features")
print(f"   ‚Ä¢ gamma: 0.0-0.15 (mod√©r√©) ‚Üí Pruning MOD√âR√â pour classe mod√©r√©e-√©lev√©e")
print(f"   ‚Ä¢ reg_alpha/lambda: 2 valeurs chacun (standard pour classe mod√©r√©e-√©lev√©e)")
print(f"   ‚Ä¢ n_estimators: 350 (r√©duit pour taille moyenne + classe mod√©r√©e-√©lev√©e)")
print(f"   ‚Ä¢ 3√ó3√ó3√ó3√ó3√ó2√ó2√ó2√ó1√ó3 = ~2,916 combinaisons (~120-180 min pour 32K lignes)")


print(f"\nüéØ === √âTAPE 2/2: FINE-TUNING PR√âCIS (100% donn√©es) ===")
print(f"Grille raffin√©e autour des meilleurs params Stage 1")
print(f"Objectif: Optimisation MAXIMALE avec toutes les donn√©es")
print(f"üìä Donn√©es Stage 2: {len(X_train):,} lignes (100% du train)")

# √âTAPE 2: Fine-tuning sur TOUTES les donn√©es d'entra√Ænement
start_stage2 = time.time()
grid_stage2 = GridSearchCV(
    model, param_grid_stage2,
    scoring=scoring_metrics,  # Toutes les m√©triques pour Stage 2
    refit='f1',
    cv=cv,
    verbose=2,
    n_jobs=n_jobs,
    return_train_score=False
)

grid_stage2.fit(X_train, y_train)  # 100% des donn√©es train
stage2_time = time.time() - start_stage2

# Le meilleur mod√®le final est celui du Stage 2
grid = grid_stage2
training_time = stage1_time + stage2_time

print(f"\n‚úÖ === RECHERCHE PAR √âTAPES TERMIN√âE ===")
print(f"Temps Stage 1: {stage1_time:.1f}s ({stage1_time/60:.1f} min)")
print(f"Temps Stage 2: {stage2_time:.1f}s ({stage2_time/60:.1f} min)")
print(f"Temps TOTAL: {training_time:.1f}s ({training_time/60:.1f} minutes)")
print(f"")
print(f"üèÜ MEILLEUR MOD√àLE FINAL (Stage 2):")
print(f"   Score F1: {grid.best_score_:.4f}")
print(f"   Param√®tres: {grid.best_params_}")
print(f"")
print(f"üìä AM√âLIORATION Stage 1 ‚Üí Stage 2:")
print(f"   Stage 1: {grid_stage1.best_score_:.4f}")
print(f"   Stage 2: {grid.best_score_:.4f}")
print(f"   Gain: {((grid.best_score_ / grid_stage1.best_score_) - 1) * 100:+.2f}%")

# Extraire et trier les r√©sultats pour obtenir le top-5
results_df = pd.DataFrame(grid.cv_results_)

# Afficher les top-5 pour diff√©rentes m√©triques
print("=== ANALYSE DES TOP-5 CONFIGURATIONS ===")

for metric in ['f1', 'precision', 'recall', 'roc_auc']:
    col_name = f'mean_test_{metric}'
    if col_name in results_df.columns:
        top_5_metric = results_df.nlargest(5, col_name)
        print(f"\nüèÜ Top 5 pour {metric.upper()} :")
        for i, (idx, row) in enumerate(top_5_metric.iterrows()):
            print(f"  {i+1}. {metric}: {row[col_name]:.4f} | Params: {row['params']}")

print(f"\nüéØ MEILLEURE CONFIGURATION :")
print(f"Param√®tres : {grid.best_params_}")
print(f"Score F1 CV : {grid.best_score_:.4f}")

# Afficher un aper√ßu du top-5 (juste pour info)
top_5_preview = results_df.nlargest(5, 'mean_test_f1')
print(f"\nüìã Aper√ßu Top-5 (sauv√© dans {DATASET_NAME}_grid_search_results.json) :")
for i, (_, row) in enumerate(top_5_preview.iterrows(), 1):
    print(f"  {i}. F1: {row['mean_test_f1']:.4f} | Params: {dict(list(row['params'].items())[:3])}...")

# V√©rifier le surapprentissage : comparer train vs test
best_model = grid.best_estimator_

# Afficher les infos d'early stopping
if hasattr(best_model, 'model_') and hasattr(best_model.model_, 'best_iteration'):
    best_iteration = best_model.model_.best_iteration
    total_estimators = best_model.n_estimators
    print(f"üõë Early Stopping : {best_iteration}/{total_estimators} estimators utilis√©s")
    print(f"   √âconomie : {total_estimators - best_iteration} estimators √©vit√©s")
y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)

from sklearn.metrics import f1_score
train_f1 = f1_score(y_train, y_pred_train)
test_f1 = f1_score(y_test, y_pred_test)

print(f"\nüìä D√âTECTION SURAPPRENTISSAGE :")
print(f"Score F1 Train : {train_f1:.4f}")
print(f"Score F1 Test  : {test_f1:.4f}")
print(f"Diff√©rence     : {abs(train_f1 - test_f1):.4f}")

if abs(train_f1 - test_f1) > 0.05:
    print("‚ö†Ô∏è  SURAPPRENTISSAGE D√âTECT√â (diff√©rence > 0.05)")
else:
    print("‚úÖ Pas de surapprentissage d√©tect√©")

# Pr√©dictions avec le meilleur mod√®le
y_pred = y_pred_test

# √âvaluation
print('\nClassification Report:')
print(classification_report(y_test, y_pred))
print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))

# Sauvegarder le meilleur mod√®le XGBoost et TOUS les r√©sultats - Nouvelle organisation
import joblib
joblib.dump(grid.best_estimator_, f'data/models/{DATASET_NAME}_xgb_model.joblib')

# Sauvegarder TOUS les r√©sultats pour diverse_top5_selector.py
results_df.to_json(f'data/results/{DATASET_NAME}_grid_search_results.json', orient='records', indent=2)

print(f'‚úÖ Mod√®le XGBoost sauvegard√© dans data/models/{DATASET_NAME}_xgb_model.joblib')
print(f'‚úÖ Tous les r√©sultats GridSearch sauvegard√©s dans data/results/{DATASET_NAME}_grid_search_results.json')
print('üéØ Prochaine √©tape: python diverse_top5_selector.py')
